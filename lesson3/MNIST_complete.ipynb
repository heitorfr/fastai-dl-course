{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is based on an exercise from Chapter 3 of the book [Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527). The chapter details the implementation of simple neural network for binary classification, using a subset of the MNIST dataset.\n\nHere we're extending that example to multi-class classification using the complete MNIST dataset. We'll use the same architectures presented in Chapter 4, which, while not state-of-the-art for the complete MNIST classification problem, serve as a valuable learning experience.\n\nFor a more accurate approach, see for instance [Beginners guide to MNIST with fast.ai](https://www.kaggle.com/code/christianwallenwein/beginners-guide-to-mnist-with-fast-ai).","metadata":{}},{"cell_type":"code","source":"#hide\n!pip install -Uqq fastai nbdev\n\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.vision.all import *\n\nimport torch\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-05T16:50:47.629050Z","iopub.execute_input":"2024-08-05T16:50:47.629509Z","iopub.status.idle":"2024-08-05T16:51:11.724100Z","shell.execute_reply.started":"2024-08-05T16:50:47.629473Z","shell.execute_reply":"2024-08-05T16:51:11.722675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start by downloading and unzipping the full MNIST dataset. The dataset is split in two folders, `training` and `testing`. Under each we have a folder for each digit `0` - `9`","metadata":{}},{"cell_type":"code","source":"path = untar_data(URLs.MNIST)\nPath.BASE_PATH = path\nsorted((path/'training').ls())","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:51:15.887592Z","iopub.execute_input":"2024-08-05T16:51:15.888102Z","iopub.status.idle":"2024-08-05T16:51:15.903946Z","shell.execute_reply.started":"2024-08-05T16:51:15.888061Z","shell.execute_reply":"2024-08-05T16:51:15.902369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll create a list of tensors, where each tensor contains the data for all the images for a given digit - each tensor has shape [N,28,28] where N is the number of images for a given digit and each image is a matrix of 28x28 pixels. For that we'll create a couple of auxiliary functions:\n\n`path_to_tensor` returns a tensor with all the images in a given path.\n\n`path_to_tensor_list` returns a list of tensors, one for each directory under `path`. Since we sort the result of `path.ls`, the tensor list will also be sorted, from digit 0 to 9. i.e. the fist list element contains the tensor with all the images of the digit 0, and so on.","metadata":{}},{"cell_type":"code","source":"def path_to_tensor(path):\n    return torch.stack([tensor(Image.open(o)) for o in path.ls()])\n    \ndef path_to_tensor_list(path):\n    return list(map(path_to_tensor, sorted(path.ls())))","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:51:18.903777Z","iopub.execute_input":"2024-08-05T16:51:18.904364Z","iopub.status.idle":"2024-08-05T16:51:18.913875Z","shell.execute_reply.started":"2024-08-05T16:51:18.904318Z","shell.execute_reply":"2024-08-05T16:51:18.912137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tensors = path_to_tensor_list(path/'training')\ntest_tensors = path_to_tensor_list(path/'testing')\n\ntrain_tensors[0].shape, test_tensors[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:51:20.887823Z","iopub.execute_input":"2024-08-05T16:51:20.888294Z","iopub.status.idle":"2024-08-05T16:51:45.988005Z","shell.execute_reply.started":"2024-08-05T16:51:20.888256Z","shell.execute_reply":"2024-08-05T16:51:45.986697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image(train_tensors[7][0])\nshow_image(test_tensors[7][0])","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:51:47.999308Z","iopub.execute_input":"2024-08-05T16:51:47.999794Z","iopub.status.idle":"2024-08-05T16:51:48.160618Z","shell.execute_reply.started":"2024-08-05T16:51:47.999759Z","shell.execute_reply":"2024-08-05T16:51:48.159162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll now concatenate all tensors to create a consolidated train and test tensor. We also change the shape - so that each image is given as a sequence of 784 pixels - and we normalize pixel values.","metadata":{}},{"cell_type":"code","source":"train_x = torch.cat(train_tensors).view(-1, 28*28).float()/255\ntest_x = torch.cat(test_tensors).view(-1, 28*28).float()/255\n\ntrain_x.shape, test_x.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:51:53.258720Z","iopub.execute_input":"2024-08-05T16:51:53.259215Z","iopub.status.idle":"2024-08-05T16:51:53.639324Z","shell.execute_reply.started":"2024-08-05T16:51:53.259171Z","shell.execute_reply":"2024-08-05T16:51:53.638267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image(train_x[13000].view(28,28))","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:51:58.297348Z","iopub.execute_input":"2024-08-05T16:51:58.297856Z","iopub.status.idle":"2024-08-05T16:51:58.369492Z","shell.execute_reply.started":"2024-08-05T16:51:58.297817Z","shell.execute_reply":"2024-08-05T16:51:58.368179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's create the target tensors using one-hot encoding. The target tensor will have 10 columns, one for each class (the digits 0 to 9). For that we'll use the `one_hot`PyTorch function.\n\nAs a first step we'll create a list of tuples containing the number of images for each digit.","metadata":{}},{"cell_type":"code","source":"train_labels = list(enumerate(map(lambda t: t.shape[0], train_tensors)))\ntrain_labels","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:02.237302Z","iopub.execute_input":"2024-08-05T16:52:02.238567Z","iopub.status.idle":"2024-08-05T16:52:02.249041Z","shell.execute_reply.started":"2024-08-05T16:52:02.238515Z","shell.execute_reply":"2024-08-05T16:52:02.247444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`labels_to_target` receives the tuple list and returns the target tensors.","metadata":{}},{"cell_type":"code","source":"def labels_to_target(labels):\n    res = torch.cat([torch.full((size,), label, dtype=torch.long) for label, size in labels])\n    res = F.one_hot(res).float()\n    return res\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:05.887396Z","iopub.execute_input":"2024-08-05T16:52:05.887849Z","iopub.status.idle":"2024-08-05T16:52:05.894799Z","shell.execute_reply.started":"2024-08-05T16:52:05.887817Z","shell.execute_reply":"2024-08-05T16:52:05.893240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y = labels_to_target(train_labels)\n\ntrain_y[torch.randperm(len(train_y))[:5]]","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:08.107697Z","iopub.execute_input":"2024-08-05T16:52:08.108168Z","iopub.status.idle":"2024-08-05T16:52:08.130298Z","shell.execute_reply.started":"2024-08-05T16:52:08.108114Z","shell.execute_reply":"2024-08-05T16:52:08.127948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y[13000]","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:14.472185Z","iopub.execute_input":"2024-08-05T16:52:14.472635Z","iopub.status.idle":"2024-08-05T16:52:14.482628Z","shell.execute_reply.started":"2024-08-05T16:52:14.472600Z","shell.execute_reply":"2024-08-05T16:52:14.481236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the same for the test data:","metadata":{}},{"cell_type":"code","source":"test_labels = list(enumerate(map(lambda t: t.shape[0], test_tensors)))\ntest_labels","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:20.474272Z","iopub.execute_input":"2024-08-05T16:52:20.475834Z","iopub.status.idle":"2024-08-05T16:52:20.486064Z","shell.execute_reply.started":"2024-08-05T16:52:20.475776Z","shell.execute_reply":"2024-08-05T16:52:20.484655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_y = labels_to_target(test_labels)\n\ntest_y[torch.randperm(len(test_y))[:5]]","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:22.844686Z","iopub.execute_input":"2024-08-05T16:52:22.845296Z","iopub.status.idle":"2024-08-05T16:52:22.858886Z","shell.execute_reply.started":"2024-08-05T16:52:22.845248Z","shell.execute_reply":"2024-08-05T16:52:22.857223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now create the data sets and loaders. We'll use a splitter to split the train set between taining and validation.","metadata":{}},{"cell_type":"code","source":"dset = list(zip(train_x,train_y))\nx,y = dset[0]\nx.shape, y","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:29.135207Z","iopub.execute_input":"2024-08-05T16:52:29.135695Z","iopub.status.idle":"2024-08-05T16:52:29.862082Z","shell.execute_reply.started":"2024-08-05T16:52:29.135648Z","shell.execute_reply":"2024-08-05T16:52:29.860667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = TrainTestSplitter(test_size=0.2, random_state=42)\ntrain_dset_indexes, val_dset_indexes = split(dset)\nlen(train_dset_indexes), len(val_dset_indexes)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:54:08.388421Z","iopub.execute_input":"2024-08-05T16:54:08.389507Z","iopub.status.idle":"2024-08-05T16:54:08.429851Z","shell.execute_reply.started":"2024-08-05T16:54:08.389450Z","shell.execute_reply":"2024-08-05T16:54:08.428433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dset = [dset[i] for i in train_dset_indexes]\nvalid_dset = [dset[i] for i in val_dset_indexes]\n\nshow_image(train_dset[0][0].view(28,28))\nprint(train_dset[0][1])","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:54:10.946698Z","iopub.execute_input":"2024-08-05T16:54:10.947831Z","iopub.status.idle":"2024-08-05T16:54:11.042420Z","shell.execute_reply.started":"2024-08-05T16:54:10.947790Z","shell.execute_reply":"2024-08-05T16:54:11.040663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dl = DataLoader(train_dset, batch_size=256, shuffle=True)\nxb,yb = first(train_dl)\nxb.shape,yb.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:54:13.304597Z","iopub.execute_input":"2024-08-05T16:54:13.305182Z","iopub.status.idle":"2024-08-05T16:54:13.388876Z","shell.execute_reply.started":"2024-08-05T16:54:13.305117Z","shell.execute_reply":"2024-08-05T16:54:13.387484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_dl = DataLoader(valid_dset, batch_size=256, shuffle=True)\nxb,yb = first(valid_dl)\nxb.shape,yb.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:54:17.079275Z","iopub.execute_input":"2024-08-05T16:54:17.080582Z","iopub.status.idle":"2024-08-05T16:54:17.124998Z","shell.execute_reply.started":"2024-08-05T16:54:17.080529Z","shell.execute_reply":"2024-08-05T16:54:17.123831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SGD\n\nThe training loop, based on the code from Chapter 4:","metadata":{}},{"cell_type":"code","source":"from torch.nn import init\n    \ndef init_lin_params(in_features, out_features, std=1.0): \n    w = (torch.randn((in_features, out_features))*std).requires_grad_()\n    b = (torch.randn(out_features)*std).requires_grad_()\n    return w, b\n\n\n# This replicates the Kaiming parameter initialization implemented in nn.Linear\ndef init_lin_params_k(in_features, out_features):\n    w = torch.empty((in_features, out_features)).requires_grad_()\n    b = torch.empty(out_features).requires_grad_()\n    init.kaiming_uniform_(w.T, a=math.sqrt(5))\n    fan_in, _ = init._calculate_fan_in_and_fan_out(w)\n    bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n    init.uniform_(b, -bound, bound)\n    return w, b\n\ndef calc_grad(xb, yb, model, lossf):\n    preds = model(xb)\n    loss = lossf(preds, yb)\n    loss.backward()\n    \ndef train_epoch(model, lr, params, lossf):\n    for xb,yb in train_dl:\n        calc_grad(xb, yb, model, lossf)\n        for p in params:\n            p.data -= p.grad.data * lr\n            p.grad.zero_()\n            \ndef batch_accuracy(xb, yb):\n    _, preds = torch.max(xb, dim=1)\n    _, target = torch.max(yb, dim=1)\n    return torch.tensor(torch.sum(preds == target).item() / len(preds))\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(tensor(accs).mean().item(), 4)\n\ndef train(model, params, lossf=F.cross_entropy, epochs=50, lr=1):\n    for i in range(epochs):\n        train_epoch(model, lr, params, lossf)\n        print(validate_epoch(model), end=' ')\n        \ndef test(model):\n    acc = batch_accuracy(model(test_x), test_y).item()\n    return f\"{acc:.2f}%\"","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:57:34.556249Z","iopub.execute_input":"2024-08-05T16:57:34.557024Z","iopub.status.idle":"2024-08-05T16:57:34.577177Z","shell.execute_reply.started":"2024-08-05T16:57:34.556984Z","shell.execute_reply":"2024-08-05T16:57:34.575831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic regression\n\nFirst let's try to train a simple logistic regression model. Note that we're using the Kaming parameter initialization as we get faster convergence relative to random initialization.","metadata":{}},{"cell_type":"code","source":"def log_reg(xb): \n    res = xb@w1 + b1\n    return res.sigmoid()\n\nw1, b1 = init_lin_params_k(28*28,10)\n\ntrain(log_reg, (w1, b1))","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:56:31.343251Z","iopub.execute_input":"2024-08-05T16:56:31.344169Z","iopub.status.idle":"2024-08-05T16:57:06.601821Z","shell.execute_reply.started":"2024-08-05T16:56:31.344104Z","shell.execute_reply":"2024-08-05T16:57:06.600600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check accuracy with the test set:","metadata":{}},{"cell_type":"code","source":"print(test(log_reg))","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:57:51.664363Z","iopub.execute_input":"2024-08-05T16:57:51.665607Z","iopub.status.idle":"2024-08-05T16:57:51.679550Z","shell.execute_reply.started":"2024-08-05T16:57:51.665554Z","shell.execute_reply":"2024-08-05T16:57:51.677852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get above 90% with a simple logistic regression model, not bad! Let's see if we get a better result using a model with two layers.","metadata":{}},{"cell_type":"markdown","source":"## Two layer net","metadata":{}},{"cell_type":"code","source":"def two_layer_net(xb): \n    res = xb@w2 + b2\n    res = res.max(tensor(0.0))\n    res = res@w3 + b3\n    res = F.softmax(res, dim=1)\n    return res\n\nw2, b2 = init_lin_params_k(28*28, 32)\nw3, b3 = init_lin_params_k(32, 10)\n\ntrain(two_layer_net, (w2, b2, w3, b3), lossf=F.cross_entropy, epochs=50)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:57:58.462239Z","iopub.execute_input":"2024-08-05T16:57:58.462649Z","iopub.status.idle":"2024-08-05T16:58:42.093579Z","shell.execute_reply.started":"2024-08-05T16:57:58.462619Z","shell.execute_reply":"2024-08-05T16:58:42.092167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this model we're getting above 95%. This is in line with the results obtained here with a similar model: [Multi-class classification with MNIST](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/multi-class_classification_with_MNIST.ipynb?hl=en#scrollTo=pedD5GhlDC-y). Let's check with the test set.","metadata":{}},{"cell_type":"code","source":"print(test(two_layer_net))","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:58:50.068136Z","iopub.execute_input":"2024-08-05T16:58:50.068877Z","iopub.status.idle":"2024-08-05T16:58:50.094407Z","shell.execute_reply.started":"2024-08-05T16:58:50.068819Z","shell.execute_reply":"2024-08-05T16:58:50.092270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using a Learner / PyTorch Modules\n\nUsing a Learner and Pytorch modules we essentially get to the same results.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\ntwo_layer_nn = nn.Sequential(\n    nn.Linear(28*28, 32),\n    nn.ReLU(),\n    nn.Linear(32,10),\n    nn.Softmax(dim=1)\n)\n\ndls = DataLoaders(train_dl, valid_dl)\n\nlearn = Learner(dls, two_layer_nn, opt_func=SGD,\n                loss_func=F.cross_entropy, metrics=batch_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:58:56.340028Z","iopub.execute_input":"2024-08-05T16:58:56.341082Z","iopub.status.idle":"2024-08-05T16:58:56.352643Z","shell.execute_reply.started":"2024-08-05T16:58:56.341026Z","shell.execute_reply":"2024-08-05T16:58:56.351330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit(50, 1)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:59:05.854122Z","iopub.execute_input":"2024-08-05T16:59:05.855339Z","iopub.status.idle":"2024-08-05T17:00:13.137178Z","shell.execute_reply.started":"2024-08-05T16:59:05.855291Z","shell.execute_reply":"2024-08-05T17:00:13.135907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test(two_layer_nn))","metadata":{"execution":{"iopub.status.busy":"2024-08-05T17:00:16.052847Z","iopub.execute_input":"2024-08-05T17:00:16.053443Z","iopub.status.idle":"2024-08-05T17:00:16.071948Z","shell.execute_reply.started":"2024-08-05T17:00:16.053396Z","shell.execute_reply":"2024-08-05T17:00:16.070550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References\n\n[Deep Learning for Coders with Fastai and PyTorch - Chapter 4](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb)\n\n[Multi-class classification with MNIST](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/multi-class_classification_with_MNIST.ipynb?hl=en#scrollTo=pedD5GhlDC-y)\n","metadata":{}}]}